# -*- coding: utf-8 -*-
"""improved_predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14xW8vgvVd_xzT-lyjsVP6kx-hL5LOizG
"""

import argparse
import json
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
from tqdm import tqdm
import re

from labels import LABEL_LIST, PII_LABELS


def validate_credit_card(text):
    """Validate credit card - should have 14-19 digit words"""
    digit_words = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'double']
    words = text.lower().split()
    digit_count = sum(1 for w in words if w in digit_words)
    # Account for 'double' reducing count
    double_count = words.count('double')
    effective_digits = digit_count + double_count
    return 14 <= effective_digits <= 19


def validate_phone(text):
    """Validate phone - should have 9-11 digit words"""
    digit_words = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'double', 'oh']
    words = text.lower().split()
    digit_count = sum(1 for w in words if w in digit_words)
    double_count = words.count('double')
    effective_digits = digit_count + double_count
    return 9 <= effective_digits <= 11


def validate_email(text):
    """Validate email - must contain 'at' and 'dot'"""
    text_lower = text.lower()
    return ('at' in text_lower or '@' in text) and ('dot' in text_lower or '.' in text)


def validate_date(text):
    """Validate date - should contain month or numeric pattern"""
    months = ['january', 'february', 'march', 'april', 'may', 'june',
              'july', 'august', 'september', 'october', 'november', 'december']
    text_lower = text.lower()

    # Check for month name
    if any(month in text_lower for month in months):
        return True

    # Check for date patterns like "DD/MM/YYYY" or "DD-MM-YYYY"
    if re.search(r'\d{1,2}[/-]\d{1,2}[/-]\d{4}', text):
        return True

    return False


def validate_person_name(text):
    """Validate person name - basic heuristics"""
    words = text.split()
    # Name should be 2-4 words, each starting with lowercase (STT format)
    if 2 <= len(words) <= 4:
        return all(w[0].islower() for w in words if w)
    return False


def decode_bio_to_spans(tokens, labels, confidence_scores, text, tokenizer, confidence_threshold=0.7):
    """Decode BIO tags to character-level spans with confidence filtering"""
    spans = []
    current_entity = None
    current_tokens = []
    current_confidences = []

    for token, label, conf in zip(tokens, labels, confidence_scores):
        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:
            continue

        if label.startswith('B-'):
            # Save previous entity
            if current_entity:
                spans.append((current_entity, current_tokens, current_confidences))

            # Start new entity
            current_entity = label[2:]
            current_tokens = [token]
            current_confidences = [conf]

        elif label.startswith('I-') and current_entity == label[2:]:
            current_tokens.append(token)
            current_confidences.append(conf)

        else:  # 'O' or mismatch
            if current_entity:
                spans.append((current_entity, current_tokens, current_confidences))
                current_entity = None
                current_tokens = []
                current_confidences = []

    # Don't forget last entity
    if current_entity:
        spans.append((current_entity, current_tokens, current_confidences))

    # Convert to character spans
    result_spans = []
    for entity_type, tokens, confidences in spans:
        # Reconstruct text from tokens
        entity_text = tokenizer.convert_tokens_to_string(tokens).strip()
        avg_confidence = sum(confidences) / len(confidences)

        # Apply confidence threshold for PII
        if entity_type in PII_LABELS and avg_confidence < confidence_threshold:
            continue

        # Pattern validation
        if entity_type == 'CREDIT_CARD' and not validate_credit_card(entity_text):
            continue
        if entity_type == 'PHONE' and not validate_phone(entity_text):
            continue
        if entity_type == 'EMAIL' and not validate_email(entity_text):
            continue
        if entity_type == 'DATE' and not validate_date(entity_text):
            continue
        if entity_type == 'PERSON_NAME' and not validate_person_name(entity_text):
            continue

        # Find character offsets in original text
        start_idx = text.lower().find(entity_text.lower())
        if start_idx != -1:
            end_idx = start_idx + len(entity_text)
            result_spans.append({
                'start': start_idx,
                'end': end_idx,
                'label': entity_type,
                'confidence': avg_confidence
            })

    return result_spans


def predict(model_dir, input_file, output_file, confidence_threshold=0.7):
    """Run inference on input file"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Load model and tokenizer
    print(f"Loading model from {model_dir}")
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForTokenClassification.from_pretrained(model_dir)
    model.to(device)
    model.eval()

    # Load input data
    print(f"Loading input from {input_file}")
    with open(input_file, 'r') as f:
        data = [json.loads(line) for line in f]

    # Run predictions
    predictions = []
    print("Running predictions...")

    with torch.no_grad():
        for item in tqdm(data):
            text = item['text']

            # Tokenize
            encoding = tokenizer(
                text,
                return_tensors='pt',
                truncation=True,
                max_length=128,
                padding=True
            )

            input_ids = encoding['input_ids'].to(device)
            attention_mask = encoding['attention_mask'].to(device)

            # Get predictions
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits[0]  # Remove batch dimension

            # Get confidence scores
            probs = torch.softmax(logits, dim=-1)
            confidences, pred_labels = torch.max(probs, dim=-1)

            # Convert to labels
            tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
            labels = [LABEL_LIST[l] for l in pred_labels.cpu().numpy()]
            confidences = confidences.cpu().numpy()

            # Decode to spans
            spans = decode_bio_to_spans(
                tokens, labels, confidences, text, tokenizer, confidence_threshold
            )

            predictions.append({
                'id': item['id'],
                'text': text,
                'entities': spans
            })

    # Save predictions
    print(f"Saving predictions to {output_file}")
    with open(output_file, 'w') as f:
        json.dump(predictions, f, indent=2)

    print(f"âœ“ Predicted {len(predictions)} utterances")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_dir', type=str, required=True)
    parser.add_argument('--input', type=str, required=True)
    parser.add_argument('--output', type=str, required=True)
    parser.add_argument('--confidence', type=float, default=0.7,
                       help='Confidence threshold for PII entities')

    args = parser.parse_args()

    predict(args.model_dir, args.input, args.output, args.confidence)


if __name__ == "__main__":
    main()