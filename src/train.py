# -*- coding: utf-8 -*-
"""improved_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1096ZPtvAA2W48iwcMDfWN21iKxxeavV0
"""

import argparse
import json
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    get_linear_schedule_with_warmup
)
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
from tqdm import tqdm
import os

from dataset import NERDataset
from labels import LABEL_LIST, PII_LABELS


def compute_label_weights(dataset):
    """Compute class weights to handle imbalance"""
    all_labels = []
    for item in dataset:
        all_labels.extend(item['labels'])

    unique_labels = sorted(set(all_labels))
    weights = compute_class_weight(
        class_weight='balanced',
        classes=np.array(unique_labels),
        y=np.array(all_labels)
    )

    # Increase weights for PII labels
    label2id = {label: idx for idx, label in enumerate(LABEL_LIST)}
    weight_dict = dict(zip(unique_labels, weights))

    # Boost PII entity weights
    for label in LABEL_LIST:
        if any(pii in label for pii in PII_LABELS):
            label_id = label2id[label]
            if label_id in weight_dict:
                weight_dict[label_id] *= 1.5  # 50% boost for PII

    return torch.tensor([weight_dict.get(i, 1.0) for i in range(len(LABEL_LIST))])


def train_epoch(model, dataloader, optimizer, scheduler, device, class_weights):
    """Train for one epoch"""
    model.train()
    total_loss = 0

    for batch in tqdm(dataloader, desc="Training"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        # Apply class weights to loss
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

    return total_loss / len(dataloader)


def evaluate(model, dataloader, device):
    """Evaluate the model"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            total_loss += outputs.loss.item()

    return total_loss / len(dataloader)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_name', type=str, default='distilbert-base-uncased')
    parser.add_argument('--train', type=str, required=True)
    parser.add_argument('--dev', type=str, required=True)
    parser.add_argument('--out_dir', type=str, required=True)
    parser.add_argument('--epochs', type=int, default=4)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--lr', type=float, default=3e-5)
    parser.add_argument('--max_length', type=int, default=128)
    parser.add_argument('--dropout', type=float, default=0.2)

    args = parser.parse_args()

    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Load tokenizer and model
    print(f"Loading model: {args.model_name}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    model = AutoModelForTokenClassification.from_pretrained(
        args.model_name,
        num_labels=len(LABEL_LIST),
        hidden_dropout_prob=args.dropout,
        attention_probs_dropout_prob=args.dropout
    )
    model.to(device)

    # Load datasets
    print("Loading datasets...")
    train_dataset = NERDataset(args.train, tokenizer, max_length=args.max_length)
    dev_dataset = NERDataset(args.dev, tokenizer, max_length=args.max_length)

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=train_dataset.collate_fn
    )
    dev_dataloader = DataLoader(
        dev_dataset,
        batch_size=args.batch_size,
        collate_fn=dev_dataset.collate_fn
    )

    # Compute class weights
    print("Computing class weights...")
    class_weights = compute_label_weights(train_dataset.data).to(device)

    # Optimizer and scheduler
    optimizer = AdamW(model.parameters(), lr=args.lr)
    total_steps = len(train_dataloader) * args.epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),
        num_training_steps=total_steps
    )

    # Training loop
    best_dev_loss = float('inf')
    print(f"\nStarting training for {args.epochs} epochs...")

    for epoch in range(args.epochs):
        print(f"\n{'='*50}")
        print(f"Epoch {epoch + 1}/{args.epochs}")
        print(f"{'='*50}")

        train_loss = train_epoch(
            model, train_dataloader, optimizer, scheduler, device, class_weights
        )
        dev_loss = evaluate(model, dev_dataloader, device)

        print(f"Train Loss: {train_loss:.4f}")
        print(f"Dev Loss: {dev_loss:.4f}")

        # Save best model
        if dev_loss < best_dev_loss:
            best_dev_loss = dev_loss
            os.makedirs(args.out_dir, exist_ok=True)
            model.save_pretrained(args.out_dir)
            tokenizer.save_pretrained(args.out_dir)
            print(f"âœ“ Saved best model (dev_loss: {dev_loss:.4f})")

    print(f"\n{'='*50}")
    print("Training complete!")
    print(f"Best dev loss: {best_dev_loss:.4f}")
    print(f"Model saved to: {args.out_dir}")


if __name__ == "__main__":
    main()